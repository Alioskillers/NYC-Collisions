# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rAa9PjNiogRd-3qC5dvC5ZIK5elGM2kW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# 1. Load the datasets

crashes_url = "https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD"
persons_url = "https://data.cityofnewyork.us/api/views/f55k-p6yu/rows.csv?accessType=DOWNLOAD"

df_crashes = pd.read_csv(crashes_url, low_memory=False)
df_persons = pd.read_csv(persons_url, low_memory=False)

print("Crashes shape:", df_crashes.shape)
print("Persons shape:", df_persons.shape)

df_crashes.head(), df_persons.head()

print(df_crashes.columns.tolist())


# CLEAN + SAFE COLUMN DETECTION 

import pandas as pd
import numpy as np

# Show actual column names 
print("=== RAW COLUMN NAMES ===")
print(df_crashes.columns.tolist())

# Normalize function
def norm(s):
    return s.strip().upper().replace(" ", "_").replace("-", "_")

# Build dictionary: normalized → original
norm_to_orig = { norm(c): c for c in df_crashes.columns }

# Candidate variants for essential columns
candidates = {
    'COLLISION_ID': ['COLLISION_ID','CRASH_ID','COLLISIONID','ACCIDENT_ID','COLLISION ID'],
    'CRASH_DATE':   ['CRASH_DATE','CRASH DATE','DATE','ACCIDENT_DATE','CRASHDATE'],
    'CRASH_TIME':   ['CRASH_TIME','CRASH TIME','TIME'],
    'LATITUDE':     ['LATITUDE','LAT','Y_COORD'],
    'LONGITUDE':    ['LONGITUDE','LON','LNG','X_COORD'],
    'BOROUGH':      ['BOROUGH','BORO']
}

def find_column(name):
    """Return original column name if found, else None."""
    for cand in candidates[name]:
        n = norm(cand)
        if n in norm_to_orig:
            return norm_to_orig[n]
    return None

# Detect matches
found = { key: find_column(key) for key in candidates }

print("\n=== DETECTED COLUMNS ===")
for k, v in found.items():
    print(f"{k}: {v}")

# Check essentials
missing = [k for k, v in found.items() if v is None and k in ['COLLISION_ID','CRASH_DATE','LATITUDE','LONGITUDE','BOROUGH']]
if missing:
    print("\n⚠ WARNING — Missing essential columns:", missing)
    print("Open Data changed column names — send me the column list above so I can map them.")
else:
    print("\nAll essential columns found.")

# Rename detected columns to standard names
rename_map = { v: k for k, v in found.items() if v is not None }
df_crashes = df_crashes.rename(columns=rename_map)

print("\n=== Columns AFTER renaming ===")
print(df_crashes.columns.tolist())

# CLEANING PERSONS

# Drop rows missing COLLISION_ID
df_persons = df_persons.dropna(subset=["COLLISION_ID"])

# Convert to integer (invalid values → NaN)
df_persons["COLLISION_ID"] = pd.to_numeric(df_persons["COLLISION_ID"], errors='coerce')
df_crashes["COLLISION_ID"] = pd.to_numeric(df_crashes["COLLISION_ID"], errors='coerce')

df_persons.dropna(subset=["COLLISION_ID"], inplace=True)

# Drop duplicates
df_persons.drop_duplicates(subset=['COLLISION_ID', 'PERSON_ID'], inplace=True)

print("Persons shape after cleaning:", df_persons.shape)

#  Robust conversion of CRASH_DATE to datetime, then extract YEAR 
import pandas as pd

# 1) quick diagnostics: show dtype and a few values
print("dtype before:", df_crashes['CRASH_DATE'].dtype)
print("sample values:", df_crashes['CRASH_DATE'].astype(str).head(10).tolist())

# 2) convert to datetime (robust): try infer, coerce bad values to NaT
df_crashes['CRASH_DATE'] = pd.to_datetime(df_crashes['CRASH_DATE'],
                                          errors='coerce',
                                          infer_datetime_format=True)

# 3) report how many failed to parse
n_total = len(df_crashes)
n_nat = df_crashes['CRASH_DATE'].isna().sum()
print(f"Parsed CRASH_DATE -> datetime: {n_total - n_nat}/{n_total} successful, {n_nat} NaT (failed)")

# try an alternate parse strategy (day-first)
if n_nat / max(n_total,1) > 0.05:
    print("Many dates failed to parse. Trying alternative parse (dayfirst=True) for remaining rows...")
    mask = df_crashes['CRASH_DATE'].isna()
    df_crashes.loc[mask, 'CRASH_DATE'] = pd.to_datetime(
        df_crashes.loc[mask, 'CRASH_DATE'].astype(str),
        errors='coerce', infer_datetime_format=True, dayfirst=True
    )
    n_nat2 = df_crashes['CRASH_DATE'].isna().sum()
    print(f"After second attempt: {n_total - n_nat2}/{n_total} parsed, {n_nat2} NaT remain")

# 4) Now it's safe to use .dt to extract year

df_crashes['YEAR'] = df_crashes['CRASH_DATE'].dt.year

# 5) Make the plot using only rows that have a YEAR
crashes_per_year = df_crashes['YEAR'].dropna().astype(int).value_counts().sort_index()

import matplotlib.pyplot as plt
plt.figure(figsize=(10,5))
plt.plot(crashes_per_year.index, crashes_per_year.values, marker='o')
plt.title("Number of Crashes per Year in NYC")
plt.xlabel("Year")
plt.ylabel("Crashes")

plt.figure(figsize=(8,5))
df_crashes['BOROUGH'].value_counts().head().plot(kind='bar')
plt.title("Top 5 Boroughs by Crash Count")
plt.xlabel("Borough")
plt.ylabel("Crash Count")
plt.show()

print(df_crashes.columns.tolist())

# Full pipeline: Download -> Clean  -> Integrate -> Save integrated.csv.gz

import pandas as pd
import numpy as np
import os
from tqdm import tqdm

# Configuration
CRASHES_URL = "https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD"
PERSONS_URL  = "https://data.cityofnewyork.us/api/views/f55k-p6yu/rows.csv?accessType=DOWNLOAD"

CLEANED_CRASHES_PATH = "/content/cleaned_crashes.csv"
CLEANED_PERSONS_PATH  = "/content/cleaned_persons.csv"
INTEGRATED_OUT = "/content/integrated.csv.gz"

# Chunk sizes 
CRASH_CHUNKSIZE = 200_000
PERSON_CHUNKSIZE = 200_000

# NYC bbox sanity filter
MIN_LAT, MAX_LAT = 40.4, 41.1
MIN_LON, MAX_LON = -74.3, -73.5

# Helper
def norm_colname(s):
    return s.strip().upper().replace(" ", "_").replace("-", "_")

def detect_and_rename_columns(df, mapping_variants):
    """
    mapping_variants: dict canonical_name -> list of variant names to try
    Returns df with detected variants renamed to canonical_name when found.
    """
    norm_to_orig = { norm_colname(c): c for c in df.columns }
    rename_map = {}
    for canon, variants in mapping_variants.items():
        for v in variants:
            n = norm_colname(v)
            if n in norm_to_orig:
                rename_map[norm_to_orig[n]] = canon
                break
    if rename_map:
        df = df.rename(columns=rename_map)
    return df, rename_map

# columns we want to canonicalize in crashes
CRASH_COL_VARIANTS = {
    "COLLISION_ID": ["COLLISION_ID", "COLLISION ID", "CRASH_ID", "CRASH ID", "UNIQUE_KEY", "UNIQUE KEY"],
    "CRASH_DATE":   ["CRASH_DATE", "CRASH DATE", "DATE"],
    "CRASH_TIME":   ["CRASH_TIME", "CRASH TIME", "TIME"],
    "LATITUDE":     ["LATITUDE", "LAT"],
    "LONGITUDE":    ["LONGITUDE", "LON", "LNG"],
    "BOROUGH":      ["BOROUGH", "BORO"],
    "NUM_PERSONS_INJURED": ["NUMBER_OF_PERSONS_INJURED", "NUMBER OF PERSONS INJURED", "PERSONS_INJURED"],
    "NUM_PERSONS_KILLED":  ["NUMBER_OF_PERSONS_KILLED",  "NUMBER OF PERSONS KILLED",  "PERSONS_KILLED"]
}
